<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="“防止以后踩坑”  pytorch 0.4 较之前api有了很大变化 0.0 Tensor在pytorch 0.4 中 Tensor 和 Variable 合并 Tensor可以理解为一个多维矩阵，可以使用 .size() 显示大小，同时也可以和 numpy.array 转换  由于和Variable合并，requires_grad 和 autograd 成了 Tensor 的属性。 Tenso">
<meta name="keywords" content="python">
<meta property="og:type" content="article">
<meta property="og:title" content="pytorch自用笔记">
<meta property="og:url" content="http://miyunluo.com/blog/2018/04/23/pytorchbg/index.html">
<meta property="og:site_name" content="miyunLuo">
<meta property="og:description" content="“防止以后踩坑”  pytorch 0.4 较之前api有了很大变化 0.0 Tensor在pytorch 0.4 中 Tensor 和 Variable 合并 Tensor可以理解为一个多维矩阵，可以使用 .size() 显示大小，同时也可以和 numpy.array 转换  由于和Variable合并，requires_grad 和 autograd 成了 Tensor 的属性。 Tenso">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-torch0.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-torch1.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-torch2.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-torch3.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-torch4.png">
<meta property="og:updated_time" content="2019-02-06T02:40:22.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="pytorch自用笔记">
<meta name="twitter:description" content="“防止以后踩坑”  pytorch 0.4 较之前api有了很大变化 0.0 Tensor在pytorch 0.4 中 Tensor 和 Variable 合并 Tensor可以理解为一个多维矩阵，可以使用 .size() 显示大小，同时也可以和 numpy.array 转换  由于和Variable合并，requires_grad 和 autograd 成了 Tensor 的属性。 Tenso">
<meta name="twitter:image" content="http://miyunluo.com/images/in-post/post-blog-torch0.png">
    
    
        
          
              <link rel="shortcut icon" href="/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/images/favicon-200x200.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>pytorch自用笔记</title>
    <!-- styles -->
    <link rel="stylesheet" href="/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/2018/04/24/DNNyoutube2016/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/2017/12/10/jun0ctf/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/blog/2018/04/23/pytorchbg/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&text=pytorch自用笔记"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&is_video=false&description=pytorch自用笔记"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch自用笔记&body=Check out this article: http://miyunluo.com/blog/2018/04/23/pytorchbg/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&name=pytorch自用笔记&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-0-Tensor"><span class="toc-number">1.</span> <span class="toc-text">0.0 Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-1-自动求导"><span class="toc-number">2.</span> <span class="toc-text">0.1 自动求导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-0-线性模型与梯度下降"><span class="toc-number">3.</span> <span class="toc-text">1.0 线性模型与梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-逻辑回归"><span class="toc-number">4.</span> <span class="toc-text">1.1 逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-MLP-Sequential-Module"><span class="toc-number">5.</span> <span class="toc-text">1.2 MLP, Sequential, Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-多分类网络"><span class="toc-number">6.</span> <span class="toc-text">1.3 多分类网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-参数初始化"><span class="toc-number">7.</span> <span class="toc-text">1.4 参数初始化</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        pytorch自用笔记
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">miyunLuo</span>
      </span>
      
    <div class="postdate">
        <time datetime="2018-04-23T07:00:00.000Z" itemprop="datePublished">2018-04-23</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/tags/python/">python</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>“防止以后踩坑”</p>
</blockquote>
<p>pytorch 0.4 较之前api有了很大变化</p>
<h2 id="0-0-Tensor"><a href="#0-0-Tensor" class="headerlink" title="0.0 Tensor"></a>0.0 Tensor</h2><p>在pytorch 0.4 中 Tensor 和 Variable 合并</p>
<p>Tensor可以理解为一个多维矩阵，可以使用 <code>.size()</code> 显示大小，同时也可以和 numpy.array 转换</p>
<p><img src="/images/in-post/post-blog-torch0.png" alt=""></p>
<p>由于和Variable合并，<code>requires_grad</code> 和 <code>autograd</code> 成了 <code>Tensor</code> 的属性。</p>
<p><code>Tensor</code> 包含的属性，<code>.data</code> 获得Tensor的数据 <code>.item</code> 获得 Tensor的数值大小，<code>.grad</code> 获得梯度，比如</p>
<p><img src="/images/in-post/post-blog-torch1.png" alt=""></p>
<p>现在Tensor要求导必须是 floating point dtype</p>
<p>以及一些其他操作</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 沿行取最大值</span></span><br><span class="line">max_value, max_idx = torch.max(x, dim=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 沿行求和</span></span><br><span class="line">sum_x = torch.sum(x, dim=<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 增减维度</span></span><br><span class="line">x = x.unsqueeze(<span class="number">0</span>) <span class="comment"># 在第一维增加</span></span><br><span class="line">x = x.unsqueeze(<span class="number">1</span>) <span class="comment"># 在第二维增加</span></span><br><span class="line"></span><br><span class="line">x = x.squeeze(<span class="number">0</span>)   <span class="comment"># 减少第一维度</span></span><br><span class="line">x = x.squeeze()    <span class="comment"># tensor中所有维度为1全部去掉</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度交换</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line"><span class="comment"># permute 重新排列</span></span><br><span class="line">x = x.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># x : torch.Size([4, 3, 5])</span></span><br><span class="line"><span class="comment"># transpose 交换 tensor 中两个维度</span></span><br><span class="line">x = x.transpose(<span class="number">0</span>, <span class="number">2</span>)  <span class="comment"># x : torch.Size([5, 3, 4])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># view 对 tensor 进行 reshape</span></span><br><span class="line">x = torch.randn(<span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>)</span><br><span class="line">x = x.view(<span class="number">-1</span>, <span class="number">5</span>) <span class="comment"># -1 表示任意大小，5 表示第二维变成 5</span></span><br><span class="line">				  <span class="comment"># x : torch.Size([12, 5])</span></span><br><span class="line">x = x.view(<span class="number">3</span>, <span class="number">20</span>) <span class="comment"># x : torch.Size([3, 20])</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># inplace 操作，直接对tensor操作而不需要另外开辟内存空间。一般是操作符后加_</span></span><br><span class="line">x.unsqueeze_(<span class="number">0</span>)     <span class="comment"># unsqueeze 进行 inplace</span></span><br><span class="line">x.transpose_(<span class="number">1</span>, <span class="number">0</span>)  <span class="comment"># transpose 进行 inplace</span></span><br><span class="line"></span><br><span class="line">x = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">y = torch.ones(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">x.add_(y)	<span class="comment"># add 进行 inplace</span></span><br></pre></td></tr></table></figure>
<h2 id="0-1-自动求导"><a href="#0-1-自动求导" class="headerlink" title="0.1 自动求导"></a>0.1 自动求导</h2><ul>
<li><p><strong>多次自动求导</strong></p>
<p>在调用过 backward 后自动计算一次导数，但是再次调用会报错，因为pytorch默认做完一次自动求导后，计算图被丢弃，两次求导需要手动设置。</p>
</li>
</ul>
<p><img src="/images/in-post/post-blog-torch2.png" alt=""></p>
<p>y对x两次求导，第一次保留计算图，第二次不保留。第二期求导后，梯度变为两次梯度的和，也就是8.2+8.2</p>
<h2 id="1-0-线性模型与梯度下降"><a href="#1-0-线性模型与梯度下降" class="headerlink" title="1.0 线性模型与梯度下降"></a>1.0 线性模型与梯度下降</h2><p>最简单的线性模型$y=x*w+b$，计算误差函数为$\frac{1}{n}\sum^n_{i=1}(\widehat{y}_i-y_i)^2$</p>
<p>数分里都学过梯度的意义在于，沿着梯度函数变化最快，为了尽快找到误差的最小值，需要沿着梯度方向更新$w,b$，二者的梯度分别为</p>
<p>$$\frac{\partial}{\partial w}=\frac{2}{n}\sum^n_{i=1}x_i(wx_i+b-y_i)$$</p>
<p>$$\frac{\partial}{\partial b}=\frac{2}{n}\sum^n_{i=1}(wx_i+b-y_i)$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 多次更新参数</span></span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">10</span>): <span class="comment"># 进行 10 次更新</span></span><br><span class="line">    y_ = linear_model(x_train)</span><br><span class="line">    loss = get_loss(y_, y_train)</span><br><span class="line">    </span><br><span class="line">    w.grad.zero_() <span class="comment"># 记得归零梯度</span></span><br><span class="line">    b.grad.zero_() <span class="comment"># 记得归零梯度</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    </span><br><span class="line">    w.data = w.data - <span class="number">1e-2</span> * w.grad.data <span class="comment"># 更新 w</span></span><br><span class="line">    b.data = b.data - <span class="number">1e-2</span> * b.grad.data <span class="comment"># 更新 b </span></span><br><span class="line">    print(<span class="string">'epoch: &#123;&#125;, loss: &#123;&#125;'</span>.format(e, loss.data))</span><br></pre></td></tr></table></figure>
<p>需要注意的是，每一次求梯度的时候，需要手动设置 <strong>梯度归零</strong>，因为pytorch默认对梯度进行累加。（但是每次bp后都会将计算图丢弃，为什么循环里没有丢弃？）</p>
<p>code 见 <a href="https://github.com/miyunluo/pytorch-beginner/blob/master/basics/sec1.0.py" target="_blank" rel="noopener">sec1.0.py</a></p>
<h2 id="1-1-逻辑回归"><a href="#1-1-逻辑回归" class="headerlink" title="1.1 逻辑回归"></a>1.1 逻辑回归</h2><p>Logistic 回归处理的是一个分类问题 (二分类)，形式与线性回归一样，都是 $y=wx+b$，但是它使用 Sigmod 函数将结果变到 0 ~ 1 之间。对于任意输入一个数据，经过 Sigmoid 之后的结果记为 $\widehat{y}$</p>
<ul>
<li>损失函数</li>
</ul>
<p>$$loss=-(ylog(\widehat{y})+(1-y)log(1-\widehat{y}))$$</p>
<p>$y$ 表示真实 label，取值 {0, 1}。如果 $y$ 是 0，表示属于第一类，则希望 $\widehat{y}$ 越小越好，这时 $loss$ 函数为，$loss=-(log(1-\widehat{y}))$，根据函数单调性，最小化 $loss$ 也就是最小化 $\widehat{y}$，与要求一致。</p>
<p>如果 $y$ 是 1，表示属于第二类，则希望 $\widehat{y}$ 越大越好，这时 $loss$ 函数为，$loss=-(log(\widehat{y}))$，最小化 $loss$ 是最大化 $\widehat{y}$。</p>
<p>使用 <code>torch.optim</code> 更新参数，需要配合另一个数据类型 <code>nn.Parameter</code> Parameter 默认要求梯度。</p>
<p>pytorch 提供了 <code>nn.BCEWithLogitsLoss()</code>，将 sigmoid 和 loss 写在了一起，直接使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment"># 前向传播</span></span><br><span class="line">    y_pred = logistic_reg(x_data)</span><br><span class="line">    loss = criterion(y_pred, y_data)</span><br><span class="line">    <span class="comment"># 反向传播</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>
<p>与前面一样，每次bp之前需要梯度归零，code见 <a href="https://github.com/miyunluo/pytorch-beginner/blob/master/basics/sec1.1.py" target="_blank" rel="noopener">sec1.1.py</a></p>
<p>一般套路是 定义一个criterion，一个optimizer，然后bp的时候先optimizer.zero_grad()，然后loss.backward()，optimizer.step()</p>
<h2 id="1-2-MLP-Sequential-Module"><a href="#1-2-MLP-Sequential-Module" class="headerlink" title="1.2 MLP, Sequential, Module"></a>1.2 MLP, Sequential, Module</h2><p><strong>MLP</strong></p>
<p>神经网络使用的激活函数都是非线性的，sigmode函数 $\sigma(x)=\frac{1}{1+e^{-x}}$，tanh函数 $tanh(x)=2\sigma(2x)-1$，ReLU函数 $ReLU(x)=max(0,x)$。其中ReLU使用的较多，因为计算简单，可以加速梯度下降的收敛速度。</p>
<ul>
<li><p>为什么要使用激活函数</p>
<p>假设一个两层的网络，不使用激活函数，结构为$y=w_2(w_1x)=(w_2w_1)x=wx$，实际和一个一层的网络一样。</p>
<p>通过使用激活函数，网络可以通过改变权重实现任意形状，成为非线性分类器。</p>
</li>
</ul>
<p><strong>Sequential</strong></p>
<p>Sequential 用来构建序列化模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># sequential</span></span><br><span class="line">seq_net = nn.Sequential(</span><br><span class="line">	nn.Linear(<span class="number">2</span>,<span class="number">4</span>),</span><br><span class="line">    nn.Tanh(),</span><br><span class="line">    nn.Linear(<span class="number">4</span>,<span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line"><span class="comment"># 序列化模型可以通过索引访问每一层</span></span><br><span class="line">seq_net[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 打印第一层权重</span></span><br><span class="line">w0 = seq_net[<span class="number">0</span>].weight</span><br><span class="line">print(w0)</span><br><span class="line"><span class="comment"># 通过parameters获得模型参数</span></span><br><span class="line">param = seq_net.parameters()</span><br><span class="line"><span class="comment"># 定义优化器</span></span><br><span class="line">optim = torch.optim.SGD(param, <span class="number">1.</span>)</span><br></pre></td></tr></table></figure>
<p>保存模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 将参数和模型保存在一起</span></span><br><span class="line">torch.save(seq_net, <span class="string">'save_seq_net.pth'</span>)</span><br><span class="line"><span class="comment"># 读取保存的模型</span></span><br><span class="line">seq_net1 = torch.load(<span class="string">'save_seq_net.pth'</span>)</span><br></pre></td></tr></table></figure>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 只保存模型参数</span></span><br><span class="line">torch.save(seq_net.state_dict(), <span class="string">'save_seq_net_params.pth'</span>)</span><br><span class="line"><span class="comment"># 要重新读入模型的参数，首先我们需要重新定义一次模型，接着重新读入参数</span></span><br><span class="line">seq_net2 = nn.Sequential(</span><br><span class="line">    nn.Linear(<span class="number">2</span>, <span class="number">4</span>),</span><br><span class="line">    nn.Tanh(),</span><br><span class="line">    nn.Linear(<span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">)</span><br><span class="line">seq_net2.load_state_dict(torch.load(<span class="string">'save_seq_net_params.pth'</span>))</span><br></pre></td></tr></table></figure>
<p><strong>Module</strong></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># module模板</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> 网络名字<span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, 一些定义的参数)</span>:</span></span><br><span class="line">        super(网络名字, self).__init__()</span><br><span class="line">        self.layer1 = nn.Linear(num_input, num_hidden)</span><br><span class="line">        self.layer2 = nn.Sequential(...)</span><br><span class="line">        ...</span><br><span class="line"></span><br><span class="line">        定义需要用的网络层</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span> <span class="comment"># 定义前向传播</span></span><br><span class="line">        x1 = self.layer1(x)</span><br><span class="line">        x2 = self.layer2(x)</span><br><span class="line">        x = x1 + x2</span><br><span class="line">        ...</span><br><span class="line">        <span class="keyword">return</span> x</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 访问模型中的某层可以直接通过名字</span></span><br><span class="line">l1 = 网络名字.layer1</span><br></pre></td></tr></table></figure>
<h2 id="1-3-多分类网络"><a href="#1-3-多分类网络" class="headerlink" title="1.3 多分类网络"></a>1.3 多分类网络</h2><p><strong>softmax</strong></p>
<p>设多分类网络输出为 $z_1,z_2,…,z_k$，首先取指数变成 $e^{z_1},e^{z_2},…,e^{z_k}$，每一项都除以求和</p>
<p>$$\frac{e^{z_i}}{\sum^k_{j=1}e^{z_j}}$$</p>
<p>于是所有项求和等于1，每一项分别表示其中某一种的概率。</p>
<p><strong>交叉熵</strong></p>
<p>多分类问题使用更加复杂的loss函数，交叉熵。</p>
<p>$$cross_entropy(p,q)=E_p[-logq]=-\frac{1}{m}p(x)logq(x)$$</p>
<p>这里需要back up一下。所谓 <strong>熵</strong> 是用来反应信息量的。</p>
<ul>
<li><p>信息量</p>
<p>假设 $X$ 是一个离散随机变量，概率分布为 $p(x)=Pr(X=x)$，事件 $X=x_0$的信息量为 $I(x_0)=-log(p(x_0))$。从表达式可以看出，概率越大，信息量越小。概率越大，大家越会认为这一事件的发生几乎是确定的，这一事件的发生并不会引入太多信息量。</p>
</li>
<li><p>熵</p>
<p>假设李华考试结果为一个0-1分布，{0不及格，1及格}，根据先验知识，由于李华不好好学习，考试及格(记为事件A)概率是$p(x_A)=0.1$，不及格的概率是0.9，则所有可能结果带来的额外信息的期望为</p>
<p>$$H_A(x)=-[p(x_A)log(p(x_A))+(1-P(x_A))log(1-p(x_A))]=0.469$$</p>
<p>如果还有一个张华成绩不好不坏，考试及格(记为时间B)概率是 $p(x_B)=0.5$</p>
<p>$$H_B(x)=-[p(x_B)log(p(x_B))+(1-P(x_B))log(1-p(x_B))]=1$$</p>
<p>说明在成绩出来前，猜出张华的结果比李华要难。熵越大，变量取值越不稳定。</p>
</li>
<li><p>交叉熵</p>
<p>两个分布$p,q$，$CEH(p,q)=E_p[-logq]$</p>
</li>
</ul>
<p>对于二分类问题，loss可以写为</p>
<p>$$-\frac{1}{m}\sum^m_{i=1}(y^ilog\, sigmod(x^i)+(1-y^i)log(1-sigmod(x^i)))$$</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># pytorch 提供了交叉熵函数</span></span><br><span class="line">criterion = nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
<h2 id="1-4-参数初始化"><a href="#1-4-参数初始化" class="headerlink" title="1.4 参数初始化"></a>1.4 参数初始化</h2><p>首先，使用Sequential构建模型，可以直接访问参数</p>
<p><img src="/images/in-post/post-blog-torch3.png" alt=""></p>
<p>那么就可以直接从numpy赋值 来初始化参数</p>
<p><img src="/images/in-post/post-blog-torch4.png" alt=""></p>
<p>使用</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net1[<span class="number">0</span>].weight.data = torch.from_numpy(np.random.uniform(<span class="number">3</span>, <span class="number">5</span>, size=(<span class="number">4</span>, <span class="number">3</span>)))</span><br></pre></td></tr></table></figure>
<p>初始化了第一层的权重，可以看到weight的数值发生了变化，bias没有变。</p>
<p>要对每一个层都初始化，使用循环</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> net1:</span><br><span class="line">    <span class="keyword">if</span> isinstance(layer, nn.Linear): <span class="comment"># 判断是否是线性层</span></span><br><span class="line">        paran_shape = layer.weight.shape</span><br><span class="line">        layer.weight.data = torch.form_numpy(np.random.normal(<span class="number">0</span>, <span class="number">0.5</span>, size=param_shape))</span><br><span class="line">        <span class="comment"># 正太分布</span></span><br></pre></td></tr></table></figure>
<p>pythorch 提供了 <code>torch.nn.init</code> 用于初始化</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torch.nn <span class="keyword">import</span> init</span><br><span class="line">init.xavier_uniform(net1[<span class="number">0</span>].weight)</span><br></pre></td></tr></table></figure>
<p>Xavier初始化方式来源于一篇论文，证明使用这种初始化方法可以使得每层的输出方差尽可能相等</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#0-0-Tensor"><span class="toc-number">1.</span> <span class="toc-text">0.0 Tensor</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#0-1-自动求导"><span class="toc-number">2.</span> <span class="toc-text">0.1 自动求导</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-0-线性模型与梯度下降"><span class="toc-number">3.</span> <span class="toc-text">1.0 线性模型与梯度下降</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-1-逻辑回归"><span class="toc-number">4.</span> <span class="toc-text">1.1 逻辑回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-2-MLP-Sequential-Module"><span class="toc-number">5.</span> <span class="toc-text">1.2 MLP, Sequential, Module</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-3-多分类网络"><span class="toc-number">6.</span> <span class="toc-text">1.3 多分类网络</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#1-4-参数初始化"><span class="toc-number">7.</span> <span class="toc-text">1.4 参数初始化</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/blog/2018/04/23/pytorchbg/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&text=pytorch自用笔记"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&is_video=false&description=pytorch自用笔记"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=pytorch自用笔记&body=Check out this article: http://miyunluo.com/blog/2018/04/23/pytorchbg/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&title=pytorch自用笔记"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/blog/2018/04/23/pytorchbg/&name=pytorch自用笔记&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Yudong Luo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/">Home</a></li>
         
          <li><a href="/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/lib/jquery/jquery.min.js"></script>
<script src="/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-102635433-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<!-- Mathjax -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


