<!DOCTYPE html>
<html>
<head>
    <!-- so meta -->
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="HandheldFriendly" content="True">
    <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1" />
    <meta name="description" content="by David Silver   0. Markov Dicison Process MDP is a tuple $&amp;lt;S,A,P,R,\gamma&amp;gt;$  $S$ is states set, $A$ is action set, $P$ is transition probability matrix, $R$ is reward function, $\gamma$ is di">
<meta name="keywords" content="RL">
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning">
<meta property="og:url" content="http://miyunluo.com/blog/2019/01/25/RLDavid/index.html">
<meta property="og:site_name" content="miyunLuo">
<meta property="og:description" content="by David Silver   0. Markov Dicison Process MDP is a tuple $&amp;lt;S,A,P,R,\gamma&amp;gt;$  $S$ is states set, $A$ is action set, $P$ is transition probability matrix, $R$ is reward function, $\gamma$ is di">
<meta property="og:locale" content="default">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-mdp_vf.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-student_mdp.png">
<meta property="og:image" content="http://miyunluo.com/images/in-post/post-blog-student_optpolicy.png">
<meta property="og:updated_time" content="2019-05-08T04:54:56.000Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="Reinforcement Learning">
<meta name="twitter:description" content="by David Silver   0. Markov Dicison Process MDP is a tuple $&amp;lt;S,A,P,R,\gamma&amp;gt;$  $S$ is states set, $A$ is action set, $P$ is transition probability matrix, $R$ is reward function, $\gamma$ is di">
<meta name="twitter:image" content="http://miyunluo.com/images/in-post/post-blog-mdp_vf.png">
    
    
        
          
              <link rel="shortcut icon" href="/blog/images/favicon.ico">
          
        
        
          
            <link rel="icon" type="image/png" href="/blog/images/favicon-200x200.png" sizes="192x192">
          
        
        
          
            <link rel="apple-touch-icon" sizes="180x180" href="/blog/images/apple-touch-icon.png">
          
        
    
    <!-- title -->
    <title>Reinforcement Learning</title>
    <!-- styles -->
    <link rel="stylesheet" href="/blog/css/style.css">
    <!-- persian styles -->
    
      <link rel="stylesheet" href="/blog/css/rtl.css">
    
    <!-- rss -->
    
    
</head>

<body class="max-width mx-auto px3 ltr">
    
      <div id="header-post">
  <a id="menu-icon" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="menu-icon-tablet" href="#"><i class="fas fa-bars fa-lg"></i></a>
  <a id="top-icon-tablet" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');" style="display:none;"><i class="fas fa-chevron-up fa-lg"></i></a>
  <span id="menu">
    <span id="nav">
      <ul>
         
          <li><a href="/blog/">Home</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
        
      </ul>
    </span>
    <br/>
    <span id="actions">
      <ul>
        
        <li><a class="icon" href="/blog/2019/11/17/VI/"><i class="fas fa-chevron-left" aria-hidden="true" onmouseover="$('#i-prev').toggle();" onmouseout="$('#i-prev').toggle();"></i></a></li>
        
        
        <li><a class="icon" href="/blog/2018/09/19/CMPT705Algori/"><i class="fas fa-chevron-right" aria-hidden="true" onmouseover="$('#i-next').toggle();" onmouseout="$('#i-next').toggle();"></i></a></li>
        
        <li><a class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up" aria-hidden="true" onmouseover="$('#i-top').toggle();" onmouseout="$('#i-top').toggle();"></i></a></li>
        <li><a class="icon" href="#"><i class="fas fa-share-alt" aria-hidden="true" onmouseover="$('#i-share').toggle();" onmouseout="$('#i-share').toggle();" onclick="$('#share').toggle();return false;"></i></a></li>
      </ul>
      <span id="i-prev" class="info" style="display:none;">Previous post</span>
      <span id="i-next" class="info" style="display:none;">Next post</span>
      <span id="i-top" class="info" style="display:none;">Back to top</span>
      <span id="i-share" class="info" style="display:none;">Share post</span>
    </span>
    <br/>
    <div id="share" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/blog/2019/01/25/RLDavid/"><i class="fab fa-facebook " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&text=Reinforcement Learning"><i class="fab fa-twitter " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-linkedin " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&is_video=false&description=Reinforcement Learning"><i class="fab fa-pinterest " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Reinforcement Learning&body=Check out this article: http://miyunluo.com/blog/2019/01/25/RLDavid/"><i class="fas fa-envelope " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-get-pocket " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-reddit " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-stumbleupon " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-digg " aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&name=Reinforcement Learning&description="><i class="fab fa-tumblr " aria-hidden="true"></i></a></li>
</ul>

    </div>
    <div id="toc">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-Markov-Dicison-Process"><span class="toc-number">1.</span> <span class="toc-text">0. Markov Dicison Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Model-Based-Learning"><span class="toc-number">2.</span> <span class="toc-text">1. Model Based Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-Policy-Iteration"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 Policy Iteration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Value-Iteration"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 Value Iteration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-Emulation"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 Emulation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Model-Free-Prediction"><span class="toc-number">3.</span> <span class="toc-text">2. Model Free Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Monte-Carlo-Learning"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Monte-Carlo Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Temporal-Difference-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 Temporal-Difference Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Model-Free-Control"><span class="toc-number">4.</span> <span class="toc-text">3. Model Free Control</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-MC-Control"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 MC Control</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Sarsa"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 Sarsa</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-Q-Learning"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Value-Function-Approximation"><span class="toc-number">5.</span> <span class="toc-text">4. Value Function Approximation</span></a></li></ol>
    </div>
  </span>
</div>

    
    <div class="content index my4">
        
        <article class="post" itemscope itemtype="http://schema.org/BlogPosting">
  <header>
    
    <h1 class="posttitle" itemprop="name headline">
        Reinforcement Learning
    </h1>



    <div class="meta">
      <span class="author" itemprop="author" itemscope itemtype="http://schema.org/Person">
        <span itemprop="name">miyunLuo</span>
      </span>
      
    <div class="postdate">
        <time datetime="2019-01-25T08:00:00.000Z" itemprop="datePublished">2019-01-25</time>
    </div>


      
    <div class="article-tag">
        <i class="fas fa-tag"></i>
        <a class="tag-link" href="/blog/tags/RL/">RL</a>
    </div>


    </div>
  </header>
  

  <div class="content" itemprop="articleBody">
    <blockquote>
<p>by David Silver </p>
</blockquote>
<h3 id="0-Markov-Dicison-Process"><a href="#0-Markov-Dicison-Process" class="headerlink" title="0. Markov Dicison Process"></a>0. Markov Dicison Process</h3><ul>
<li>MDP is a tuple $&lt;S,A,P,R,\gamma&gt;$</li>
</ul>
<p>$S$ is states set, $A$ is action set, $P$ is transition probability matrix, $R$ is reward function, $\gamma$ is discount factor.</p>
<ul>
<li>Value function is </li>
</ul>
<p><img src="/images/in-post/post-blog-mdp_vf.png" alt="img"></p>
<ul>
<li>Emulation (try Student MDP example given on David’s MDP slides)</li>
</ul>
<p><img src="/images/in-post/post-blog-student_mdp.png" alt="img"></p>
<ul>
<li><p>Code is <code>/Funny_Tools_and_Demo/MDP/mdp.py</code>  using python3. (on github)</p>
</li>
<li><p>Results</p>
</li>
</ul>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">v(FB): -2.318673</span><br><span class="line">v(C1): -1.325513</span><br><span class="line">v(C2): 2.699597</span><br><span class="line">v(C3): 7.363954</span><br><span class="line">v(Sleep): 0.000000</span><br></pre></td></tr></table></figure>
<h3 id="1-Model-Based-Learning"><a href="#1-Model-Based-Learning" class="headerlink" title="1. Model Based Learning"></a>1. Model Based Learning</h3><p>Know the mdp behand. Try to find the optimal policy. Two approach: Policy Iteration and Value Iteration.</p>
<h4 id="1-1-Policy-Iteration"><a href="#1-1-Policy-Iteration" class="headerlink" title="1.1 Policy Iteration"></a>1.1 Policy Iteration</h4><ul>
<li>First, <strong>Policy Evaluation</strong>. Initialize a policy, calculate value for each state under this policy.<ul>
<li>Using Bellman Equation in Section 0.</li>
</ul>
</li>
<li>Second, <strong>Policy Improvement</strong>. If value becomes bigger taking another action, update the policy.</li>
</ul>
<h4 id="1-2-Value-Iteration"><a href="#1-2-Value-Iteration" class="headerlink" title="1.2 Value Iteration"></a>1.2 Value Iteration</h4><p>Calculate the value based on the next step, compare all the possible actions and choose the best one based on the current situation.</p>
<h4 id="1-3-Emulation"><a href="#1-3-Emulation" class="headerlink" title="1.3 Emulation"></a>1.3 Emulation</h4><ul>
<li>Choose from David’s MDP slides</li>
</ul>
<p><img src="/images/in-post/post-blog-student_optpolicy.png" alt="img"></p>
<ul>
<li>Code is <code>/Funny_Tools_and_Demo/MDP/policy_iter.py</code> and <code>/Funny_Tools_and_Demo/MDP/value_iter.py</code></li>
</ul>
<h3 id="2-Model-Free-Prediction"><a href="#2-Model-Free-Prediction" class="headerlink" title="2. Model Free Prediction"></a>2. Model Free Prediction</h3><p>We do not konw the transition probability and reward function in MDP, and we want to calculate the state values.</p>
<h4 id="2-1-Monte-Carlo-Learning"><a href="#2-1-Monte-Carlo-Learning" class="headerlink" title="2.1 Monte-Carlo Learning"></a>2.1 Monte-Carlo Learning</h4><p>Get samples</p>
<p>$s_1,a_1,r_1,…,s_k,a_k,r_k\sim\pi$</p>
<p>First visit or every visit state $s$</p>
<p>$g_s=r_t+\gamma%20r_{t+1}+…+\gamma^{k-t}r_k$</p>
<p>Refresh value</p>
<p>$S(s)=S(s)+g_s$</p>
<p>$N(s)=N(s)+1$</p>
<p>$v(s)=\frac{S(s)}{N(s)}$</p>
<p>Code</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">### state_sample, action_sample, reward_sample contains several lists, each list is a sample</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mc</span><span class="params">(gamma, state_sample, action_sample, reward_sample)</span>:</span></span><br><span class="line">    vfunc = dict()</span><br><span class="line">    nfunc = dict()</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        vfunc[s] = <span class="number">0.0</span></span><br><span class="line">        nfunc[s] = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">for</span> iter <span class="keyword">in</span> range(len(state_sample)):</span><br><span class="line">        G = <span class="number">0.0</span></span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(len(state_sample[iter])<span class="number">-1</span>,  <span class="number">-1</span>,  <span class="number">-1</span>):</span><br><span class="line">            G *= gamma</span><br><span class="line">            G += reward_sample[iter][step]</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> range(len(state_sample[iter])):</span><br><span class="line">            s = state_sample[iter][step]</span><br><span class="line">            vfunc[s] += G</span><br><span class="line">            nfunc[s] += <span class="number">1.0</span></span><br><span class="line">            G -= reward_sample[iter][step]</span><br><span class="line">            G /= gamma</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        <span class="keyword">if</span> nfunc[s] &gt; <span class="number">0.000001</span>:</span><br><span class="line">            vfunc[s] /= nfunc[s]</span><br></pre></td></tr></table></figure>
<h4 id="2-2-Temporal-Difference-Learning"><a href="#2-2-Temporal-Difference-Learning" class="headerlink" title="2.2 Temporal-Difference Learning"></a>2.2 Temporal-Difference Learning</h4><p>Do not need to reach the end of the MDP chain. Renew value at each step.</p>
<p>$v(s)=v(s)+\alpha(r+\gamma v(s’)-v(s))$</p>
<p>Code</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">td</span><span class="params">(alpha, gamma, state_sample, action_sample, reward_sample)</span>:</span></span><br><span class="line">    vfunc = dict()</span><br><span class="line">    <span class="keyword">for</span> s <span class="keyword">in</span> states:</span><br><span class="line">        vfunc[s] = <span class="number">0.0</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> iter <span class="keyword">in</span> range(len(state_sample)):</span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> range(len(state_sample[iter])):</span><br><span class="line">                s = state_sample[iter][step]</span><br><span class="line">                r = reward_sample[iter][step]</span><br><span class="line">                <span class="keyword">if</span> step &lt; ( len(state_sample[iter]) - <span class="number">1</span> ):</span><br><span class="line">                    next_s = state_sample[iter][step+<span class="number">1</span>]</span><br><span class="line">                    next_v = vfunc[next_s]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    next_v = <span class="number">0.0</span></span><br><span class="line">                vfunc[s] += alpha * (r + gamma*next_v - vfunc[s])</span><br></pre></td></tr></table></figure>
<h3 id="3-Model-Free-Control"><a href="#3-Model-Free-Control" class="headerlink" title="3. Model Free Control"></a>3. Model Free Control</h3><p>MDP is unknown and we want to find the optimal policy.</p>
<h4 id="3-1-MC-Control"><a href="#3-1-MC-Control" class="headerlink" title="3.1 MC Control"></a>3.1 MC Control</h4><p>Keep value of $q(s,a)$ and visit time $n(s,a)$, use samples to update value</p>
<p>$q(s,a)=\frac{q(s,a)*n(s,a)+g}{n(s,a)+1}$</p>
<p>$n(s,a)=n(s,a)+1$</p>
<p>where $g=r_t+\gamma%20r_{t+1}+…$</p>
<blockquote>
<p> GLIE (Greddy in LImit with Infinite Exploration)</p>
<ul>
<li>All state-action pairs are explored infinitely many times</li>
<li>The policy converges on a greddy polic</li>
</ul>
</blockquote>
<ul>
<li>Theorem</li>
</ul>
<p>GLIE Monte-Carlo control converges to the optimal action-value function</p>
<h4 id="3-2-Sarsa"><a href="#3-2-Sarsa" class="headerlink" title="3.2 Sarsa"></a>3.2 Sarsa</h4><p>State Action Reward State Action (Sarsa). It is like TD, but works on $q(s,a)$ function.</p>
<p>$q(s,a)=q(s,a)+\alpha(r+\gamma q(s’,a’)-q(s,a))$</p>
<p>$s$ is the current state, $a$ is the current action, $s’$ is the next state, $a’$ is the next action, $r$ is the reward, $\alpha$ is the learning rate.</p>
<ul>
<li>Theorem</li>
</ul>
<p>Sarse converges to the optimal action-value function under the conditions:</p>
<ol>
<li>GLIE sequence of policies</li>
<li>Robbind-Monro sequence of step-sizes</li>
</ol>
<h4 id="3-3-Q-Learning"><a href="#3-3-Q-Learning" class="headerlink" title="3.3 Q-Learning"></a>3.3 Q-Learning</h4><p>Quite similar but different from Sarsa</p>
<p>$q(s,a)=q(s,a)+\alpha(r+\gamma max q(s’,a’)-q(s,a))$</p>
<ul>
<li>Theorem</li>
</ul>
<p>Q-Learning control converges to the optimal action-value function.</p>
<h3 id="4-Value-Function-Approximation"><a href="#4-Value-Function-Approximation" class="headerlink" title="4. Value Function Approximation"></a>4. Value Function Approximation</h3><p>In real problem, state space is very large and can not go through all states. Extract features to handle this issue. Extrac feature !$\hat{s})$for state $s$ , the q function is $q(\hat{s},a)$, define a parameter $w$ </p>
<p>We want to learn the $q(\hat{s},a,w)$ to be as close to optiaml $q(s,a)$, which means to learn $w$.</p>
<p>However the optimal $q$ is unknow, use the estimated $q$ instead, three way</p>
<p>$qfunc=g_t MC$</p>
<p>$qfunc=r+\gamma q(\hat{s},a) SARSA$</p>
<p>$qfunc=r+max \gamma q(\hat{s’},a) QLearning$</p>

  </div>
</article>



    </div>
    
      <div id="footer-post-container">
  <div id="footer-post">

    <div id="nav-footer" style="display: none">
      <ul>
         
          <li><a href="/blog/">Home</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
        
      </ul>
    </div>

    <div id="toc-footer" style="display: none">
      <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#0-Markov-Dicison-Process"><span class="toc-number">1.</span> <span class="toc-text">0. Markov Dicison Process</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-Model-Based-Learning"><span class="toc-number">2.</span> <span class="toc-text">1. Model Based Learning</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-1-Policy-Iteration"><span class="toc-number">2.1.</span> <span class="toc-text">1.1 Policy Iteration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-2-Value-Iteration"><span class="toc-number">2.2.</span> <span class="toc-text">1.2 Value Iteration</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#1-3-Emulation"><span class="toc-number">2.3.</span> <span class="toc-text">1.3 Emulation</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-Model-Free-Prediction"><span class="toc-number">3.</span> <span class="toc-text">2. Model Free Prediction</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#2-1-Monte-Carlo-Learning"><span class="toc-number">3.1.</span> <span class="toc-text">2.1 Monte-Carlo Learning</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-2-Temporal-Difference-Learning"><span class="toc-number">3.2.</span> <span class="toc-text">2.2 Temporal-Difference Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-Model-Free-Control"><span class="toc-number">4.</span> <span class="toc-text">3. Model Free Control</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#3-1-MC-Control"><span class="toc-number">4.1.</span> <span class="toc-text">3.1 MC Control</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-2-Sarsa"><span class="toc-number">4.2.</span> <span class="toc-text">3.2 Sarsa</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-3-Q-Learning"><span class="toc-number">4.3.</span> <span class="toc-text">3.3 Q-Learning</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-Value-Function-Approximation"><span class="toc-number">5.</span> <span class="toc-text">4. Value Function Approximation</span></a></li></ol>
    </div>

    <div id="share-footer" style="display: none">
      <ul>
  <li><a class="icon" href="http://www.facebook.com/sharer.php?u=http://miyunluo.com/blog/2019/01/25/RLDavid/"><i class="fab fa-facebook fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://twitter.com/share?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&text=Reinforcement Learning"><i class="fab fa-twitter fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.linkedin.com/shareArticle?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-linkedin fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://pinterest.com/pin/create/bookmarklet/?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&is_video=false&description=Reinforcement Learning"><i class="fab fa-pinterest fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="mailto:?subject=Reinforcement Learning&body=Check out this article: http://miyunluo.com/blog/2019/01/25/RLDavid/"><i class="fas fa-envelope fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="https://getpocket.com/save?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-get-pocket fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://reddit.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-reddit fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.stumbleupon.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-stumbleupon fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://digg.com/submit?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&title=Reinforcement Learning"><i class="fab fa-digg fa-lg" aria-hidden="true"></i></a></li>
  <li><a class="icon" href="http://www.tumblr.com/share/link?url=http://miyunluo.com/blog/2019/01/25/RLDavid/&name=Reinforcement Learning&description="><i class="fab fa-tumblr fa-lg" aria-hidden="true"></i></a></li>
</ul>

    </div>

    <div id="actions-footer">
        <a id="menu" class="icon" href="#" onclick="$('#nav-footer').toggle();return false;"><i class="fas fa-bars fa-lg" aria-hidden="true"></i> Menu</a>
        <a id="toc" class="icon" href="#" onclick="$('#toc-footer').toggle();return false;"><i class="fas fa-list fa-lg" aria-hidden="true"></i> TOC</a>
        <a id="share" class="icon" href="#" onclick="$('#share-footer').toggle();return false;"><i class="fas fa-share-alt fa-lg" aria-hidden="true"></i> Share</a>
        <a id="top" style="display:none" class="icon" href="#" onclick="$('html, body').animate({ scrollTop: 0 }, 'fast');"><i class="fas fa-chevron-up fa-lg" aria-hidden="true"></i> Top</a>
    </div>

  </div>
</div>

    
    <footer id="footer">
  <div class="footer-left">
    Copyright &copy; 2020 Yudong Luo
  </div>
  <div class="footer-right">
    <nav>
      <ul>
         
          <li><a href="/blog/">Home</a></li>
         
          <li><a href="/blog/archives/">Writing</a></li>
        
      </ul>
    </nav>
  </div>
</footer>

</body>
</html>
<!-- styles -->
<link rel="stylesheet" href="/blog/lib/font-awesome/css/fontawesome-all.min.css">
<link rel="stylesheet" href="/blog/lib/justified-gallery/css/justifiedGallery.min.css">

<!-- jquery -->
<script src="/blog/lib/jquery/jquery.min.js"></script>
<script src="/blog/lib/justified-gallery/js/jquery.justifiedGallery.min.js"></script>
<script src="/blog/js/main.js"></script>
<!-- search -->

<!-- Google Analytics -->

    <script type="text/javascript">
        (function(i,s,o,g,r,a,m) {i['GoogleAnalyticsObject']=r;i[r]=i[r]||function() {
        (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
        m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
        })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
        ga('create', 'UA-102635433-1', 'auto');
        ga('send', 'pageview');
    </script>

<!-- Baidu Analytics -->

<!-- Disqus Comments -->


<!-- Mathjax -->

  <script type="text/x-mathjax-config">
    MathJax.Hub.Config({
      tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
      }
    });
  </script>
  <script type="text/x-mathjax-config">
    MathJax.Hub.Queue(function() {
      var all = MathJax.Hub.getAllJax(), i;
      for (i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
      }
    });
  </script>
  <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>


